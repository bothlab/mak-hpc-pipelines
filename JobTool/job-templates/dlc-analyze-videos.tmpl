#!/bin/bash
#
########## Begin Slurm header ##########
#
# Request number of nodes and CPU cores per node for job
#SBATCH --nodes=1
#SBATCH --partition=single
#SBATCH --gres=gpu:1
#
# Memory requirement
#SBATCH --mem=230gb
#
# Estimated wallclock time for job
#SBATCH --time=4:00:00
#
# Reserve node for the job exclusively
#SBATCH --exclusive
#
########### End Slurm header ##########

# Determine our log file name
logfile_basename="$SLURM_JOB_NAME.log"
if [ -z "$SLURM_JOB_NAME" ]
then
    logfile_basename="tmp.log"
fi
set -e

# Make & go to submit directory
mkdir -p {{ SDS_ROOT }}/HPC/Logs/dlc-track
cd {{ SDS_ROOT }}/HPC/Logs/dlc-track

# Redirect log output, override any old log
RESULTS_DIR={{ RESULTS_DIR }}
mkdir -p "$RESULTS_DIR"
exec > "$RESULTS_DIR/$logfile_basename" 2>&1
ln -srf "$RESULTS_DIR/$logfile_basename"

# Print some interesting job information
printf "Log for $SLURM_JOB_NAME at `date`\n\n"
echo "Submit Directory:                  $SLURM_SUBMIT_DIR"
echo "Working Directory:                 $PWD"
echo "Node name:                         $SLURMD_NODENAME"
echo "Job id:                            $SLURM_JOB_ID"
echo "Job name:                          $SLURM_JOB_NAME"
echo "Number of nodes allocated for job: $SLURM_JOB_NUM_NODES"
printf "\n\n"

if [ -z "$JOB_NO_SLURM" ]
then
    # prepare environment if we are on a cluster node
    source $(ws_find conda)/conda/etc/profile.d/conda.sh

    # Load CUDA & related modules
    module load devel/cuda/11.6
    module load lib/cudnn/8.5.0-cuda-11.6
else
    source /opt/conda/etc/profile.d/conda.sh
fi
# Enter DeepLabCut Conda environment
conda activate deeplabcut

# Run pipeline
exec {{ SDS_ROOT }}/HPC/Pipelines/dlc-analyze-videos.py \
        -c {{ CONFIG_FNAME }} \
        -d {{ RESULTS_DIR }} \
        {{ VIDEO_FILES|join(' ') }}
