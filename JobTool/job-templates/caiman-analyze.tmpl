#!/bin/bash
#
########## Begin Slurm header ##########
#
# Request number of nodes and CPU cores per node for job
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=24
#SBATCH --partition=single
#
# Memory requirement
#SBATCH --mem=380000mb
#
# Estimated wallclock time for job
{% if WRITE_VIDEO %}#SBATCH --time=12:00:00{% else %}#SBATCH --time=6:00:00{% endif %}
#
# Reserve node for the job exclusively
#SBATCH --exclusive
#
########### End Slurm header ##########

# NOTE: This could possibly run on the GPU with:
# --partition=gpu-single
# --gres=gpu:1
# --mem=382000mb
# (Needs testing!)
# --tmp=460000mb

# Determine our log file name
logfile_basename="$SLURM_JOB_NAME.log"
if [ -z "$SLURM_JOB_NAME" ]
then
    logfile_basename="tmp.log"
fi
set -e

# Go to submit directory
mkdir -p {{ SDS_ROOT }}/HPC/Logs/caiman-cnmfe
cd {{ SDS_ROOT }}/HPC/Logs/caiman-cnmfe

# Redirect log output, override any old log
mkdir -p {{ RESULTS_DIR }}
exec > "{{ RESULTS_DIR }}/$logfile_basename" 2>&1
ln -srf "{{ RESULTS_DIR }}/$logfile_basename"

# Print some interesting job information
printf "Log for $SLURM_JOB_NAME at `date`\n\n"
echo "Submit Directory:                  $SLURM_SUBMIT_DIR"
echo "Working Directory:                 $PWD"
echo "Node name:                         $SLURMD_NODENAME"
echo "Job id:                            $SLURM_JOB_ID"
echo "Job name:                          $SLURM_JOB_NAME"
echo "Number of nodes allocated for job: $SLURM_JOB_NUM_NODES"
echo "Number of requested tasks for job: $SLURM_NTASKS"
printf "\n\n"

# Enter CaImAn Conda environment
source $(ws_find conda)/conda/etc/profile.d/conda.sh
conda activate caiman

# Load CUDA
module load devel/cuda/11.2
module load lib/cudnn/8.1.1-cuda-11.2

# Run pipeline
exec {{ SDS_ROOT }}/HPC/Pipelines/caiman-cnmf-e.py \
        -d {{ RESULTS_DIR }} {% if WRITE_VIDEO %}--write-video {% endif %} \
        {{ RAW_VIDEO_FILES|join(' ') }}
